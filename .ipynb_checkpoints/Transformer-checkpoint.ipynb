{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f691a47-4ac8-4ec3-8a8f-822b95679794",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.tx\n",
    "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf6f1b-5aca-40c6-86e5-7a52be048c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60218350-87f6-425c-a6b3-489b602b98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some convenience helper functions used throughout the notebook\n",
    "\n",
    "\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a81077c-6edf-40ec-8e50-181cd3988c08",
   "metadata": {},
   "source": [
    "Encoder-Decoder Structure: The neural model consists of two main components: an encoder and a decoder. This structure is widely used in competitive sequence transduction models.\n",
    "Encoder: The encoder takes an input sequence of symbol representations and maps it to a sequence of continuous representations. Essentially, it processes the input sequence and transforms it into a form that captures meaningful information about the input.\n",
    "Decoder: The decoder takes the continuous representations produced by the encoder and generates an output sequence of symbols one element at a time. It does this by consuming the previously generated symbols as additional input when generating the next one. This auto-regressive property means that the model conditions its predictions on its previous outputs during generation.\n",
    "Auto-regressive Generation: At each step of decoding, the model generates the next symbol in the output sequence based on the previously generated symbols. This is achieved through an auto-regressive process, where the model's predictions are influenced by its own outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d005f3f-207a-4486-8288-7fcb2dce4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed #Source\n",
    "        self.tgt_embed = tgt_embed #Target embedding\n",
    "        self.generator = generator #Generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0fc33-3bff-461d-a94a-374f53c3242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c199f-3b1c-4bf8-9489-b227e44c5802",
   "metadata": {},
   "source": [
    "Encoder:\n",
    "\n",
    "-Purpose: The encoder takes the input sequence and processes it, capturing its important features.\n",
    "-Structure: It consists of multiple identical layers stacked on top of each other.\n",
    "-Layer: Each layer within the encoder has two main parts:\n",
    "--Self-Attention Mechanism: This part helps the model focus on different parts of the input sequence while processing each token.\n",
    "--Feed-Forward Network: After self-attention, each token's representation is further refined using a simple neural network.\n",
    "-Residual Connections: Each layer has a \"shortcut\" connection called a residual connection. It helps in better information flow through the network by adding the input of the layer to its output.\n",
    "-Layer Normalization: After each sub-layer (self-attention and feed-forward network), the output is normalized to stabilize training and improve performance.\n",
    "-Dropout: Dropout is applied to the output of each sub-layer to prevent overfitting.\n",
    "-Implementation: The encoder is implemented as a stack of these layers, each followed by normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d070719-c2b4-4570-8a98-6c127548106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The encoder is composed of a stack of N=6 identical layers.\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192701d6-9315-4c95-a3b2-26e966f3b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae62a6-dc87-4342-8206-57b6a6249921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0a0ca-cc6a-4c5e-8ddf-c038d43837cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5426cb-2678-4a5b-98fe-8d8128ed4afc",
   "metadata": {},
   "source": [
    "Decoder:\n",
    "\n",
    "-Purpose: The decoder takes the processed information from the encoder and generates the output sequence.\n",
    "-Structure: Similar to the encoder, it consists of multiple identical layers stacked on top of each other.\n",
    "-Layer: Each layer within the decoder also has multiple sub-layers:\n",
    "--Self-Attention Mechanism: This helps the decoder focus on different parts of the output sequence while generating each token.\n",
    "--Source-Target Attention Mechanism: This sub-layer allows the decoder to consider the information from the input sequence (memory) while generating the output sequence.\n",
    "--Feed-Forward Network: Similar to the encoder, this part refines the representation of each token.\n",
    "-Residual Connections: Just like the encoder, each layer in the decoder has residual connections for better information flow.\n",
    "-Masking: The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions. This ensures that during training, predictions for a token only depend on the tokens before it, preventing information leakage from future tokens.\n",
    "-Implementation: The decoder is also implemented as a stack of these layers, with each layer followed by normalization and dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b732ff-1088-4801-b4f5-5ed9b4d0d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e56e10-848b-4c3b-ad0b-a885c151513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bf130-3d3e-4c8b-b67b-69ad1cc24477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194ccca-13c8-409f-9cbd-021dedb2da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mask():\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(20)\n",
    "            for x in range(20)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "            alt.X(\"Window:O\"),\n",
    "            alt.Y(\"Masking:O\"),\n",
    "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232a6f6-bda5-42cd-b080-fb709bbb12e1",
   "metadata": {},
   "source": [
    "Attention Mechanism:\n",
    "\n",
    "-The attention function described maps a query and a set of key-value pairs to an output.\n",
    "-Each of these (query, keys, values, and output) are vectors.\n",
    "-The output is computed as a weighted sum of the values, where the weights are determined by a compatibility function (often referred to as a scoring function) of the query with the corresponding key.\n",
    "\n",
    "Scaled Dot-Product Attention:\n",
    "\n",
    "-This is the specific attention mechanism used in Transformers.\n",
    "-It involves queries, keys, and values, each of a certain dimension.\n",
    "-The attention scores are computed by taking the dot product of the query with all keys, scaled by the square root of the dimension of the keys.\n",
    "-Then, softmax is applied to obtain the weights on the values.\n",
    "-This mechanism allows the model to focus on different parts of the input sequence differently, based on the similarity between the query and keys.\n",
    "\n",
    "Additive Attention vs. Dot-Product Attention:\n",
    "\n",
    "-Two common types of attention are discussed: additive and dot-product.\n",
    "-Additive attention uses a feed-forward network with a single hidden layer to compute compatibility.\n",
    "-Dot-product attention, on the other hand, is faster and more space-efficient, as it utilizes matrix multiplication.\n",
    "-While both have similar theoretical complexity, dot-product attention is preferred due to its efficiency, especially for larger dimensions.\n",
    "\n",
    "Multi-Head Attention:\n",
    "\n",
    "-Multi-head attention allows the model to jointly attend to information from different representation subspaces.\n",
    "-With multiple attention heads, the model can capture different aspects of the input simultaneously.\n",
    "-Each attention head operates on a transformed version of the input (query, key, value) and produces its own output.\n",
    "-These outputs are then concatenated and linearly transformed to produce the final output of the multi-head attention layer.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "-The MultiHeadedAttention class is provided as an implementation of multi-head attention in PyTorch.\n",
    "-It takes in the number of heads (h) and the model dimension (d_model).\n",
    "-The model dimension is divided equally among the heads.\n",
    "-Linear projections are applied to the input query, key, and value to project them into the required dimensions for each head.\n",
    "-Attention is then applied independently for each head, and the outputs are concatenated and linearly transformed to produce the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26b6b1-77e0-4a43-98dd-30049e0f537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1523d-13c0-497c-bb7b-e0368a0ae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9c577-b8a7-4d01-bc56-984e58a541af",
   "metadata": {},
   "source": [
    "The applications of attention in the Transformer model are crucial for enabling it to effectively capture dependencies between input and output sequences. Here's a breakdown of how attention is utilized in different parts of the model:\n",
    "\n",
    "-Encoder-Decoder Attention:\n",
    "\n",
    "This attention mechanism allows the decoder to focus on relevant parts of the input sequence when generating each token of the output sequence.\n",
    "Queries are derived from the previous decoder layer, while keys and values are obtained from the output of the encoder.\n",
    "By enabling every position in the decoder to attend over all positions in the input sequence, this mechanism facilitates the alignment of input and output sequences, which is essential for tasks like machine translation.\n",
    "This mimics traditional encoder-decoder attention mechanisms found in sequence-to-sequence models.\n",
    "\n",
    "-Self-Attention in the Encoder:\n",
    "\n",
    "Self-attention layers in the encoder enable each position in the encoder to attend to all positions in the previous layer of the encoder.\n",
    "This allows the model to capture dependencies between different parts of the input sequence, irrespective of their relative positions.\n",
    "By allowing every position in the encoder to attend to every other position, the model can learn representations that effectively capture the relationships between different tokens in the input sequence.\n",
    "\n",
    "-Self-Attention in the Decoder:\n",
    "\n",
    "Similar to the encoder, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n",
    "However, to maintain the auto-regressive property and prevent information leakage from future tokens, leftward information flow in the decoder must be restricted.\n",
    "This is achieved by masking out (setting to -∞) all values in the input of the softmax corresponding to illegal connections, ensuring that each position attends only to positions preceding it in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055a3a1-c2e5-448f-a986-303835ab0572",
   "metadata": {},
   "source": [
    "Feed Forward Network\n",
    "\n",
    "-The position-wise feed-forward network (FFN) consists of two linear transformations with a ReLU activation function applied in between.\n",
    "-It is applied independently to each position in the input sequence.\n",
    "-The purpose of this network is to learn complex, non-linear transformations of the input representations at each position.\n",
    "\n",
    "Equation:\n",
    "\n",
    "Mathematically, the FFN can be represented as:\n",
    "FFN(𝑥)=ReLU(𝑥𝑊1+𝑏1)𝑊2+𝑏2\n",
    "where:\n",
    "𝑥 is the input vector at each position.\n",
    "𝑊1 and 𝑊2 are weight matrices for the linear transformations.\n",
    "𝑏1 and 𝑏2 are bias vectors.\n",
    "ReLU denotes the rectified linear unit activation function.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "The input dimensionality (𝑑 model) and output dimensionality (𝑑 model) of the FFN are typically set to 512.\n",
    "The inner-layer dimensionality (𝑑ff) is set to 2048, making the FFN wider than the input and output layers.\n",
    "Dropout is applied with a specified dropout rate to prevent overfitting during training.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "-In code, the FFN is implemented as a PyTorch module (PositionwiseFeedForward) with two linear layers (self.w_1 and self.w_2) and a dropout layer.\n",
    "-The ReLU activation function is applied after the first linear transformation, followed by dropout.\n",
    "-The output of the second linear transformation is the final output of the FFN.\n",
    "-In summary, the position-wise feed-forward network in the Transformer model acts as a non-linear transformation layer, enhancing the model's ability to capture complex relationships within the input sequence. It is applied independently to each position, providing flexibility and expressive power to the model's representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a0a0b-779d-4964-b5fc-4ace1b4224aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f645638-4d64-463f-944c-d34319970c92",
   "metadata": {},
   "source": [
    "Embeddings:\n",
    "\n",
    "-Learned embeddings are used to convert input tokens and output tokens into fixed-size vectors of dimension d model.\n",
    "-The Embeddings class implements this functionality in the Transformer model.\n",
    "-It utilizes an embedding layer (self.lut) initialized with random weights and learns the embeddings during training.\n",
    "-The size of the embedding layer is determined by the vocabulary size (vocab) and the model dimension (d_model).\n",
    "-During the forward pass, the input tokens are passed through the embedding layer, resulting in vectors of dimension d model.\n",
    "-Additionally, the embeddings are scaled by a factor of d model\n",
    "-This scaling helps stabilize the gradients during training.\n",
    "\n",
    "Softmax:\n",
    "\n",
    "-Softmax function is used to convert the decoder output into predicted probabilities over the vocabulary for the next token.\n",
    "-This is a standard practice in sequence transduction models.\n",
    "-The output of the model is passed through a linear transformation followed by the softmax function to obtain token probabilities.\n",
    "\n",
    "Parameter Sharing:\n",
    "\n",
    "-The weights of the embedding layers and the pre-softmax linear transformation are shared.\n",
    "-This means that the same weight matrix is used for both embedding layers and the linear transformation layer.\n",
    "-Parameter sharing helps reduce the number of parameters in the model and encourages better generalization.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "-The Embeddings class inherits from nn.Module in PyTorch and implements the forward method.\n",
    "-During initialization, it creates an embedding layer (self.lut) with dimensions specified by the vocabulary size and the model dimension.\n",
    "-During the forward pass, input tokens are passed through the embedding layer and scaled by d model.\n",
    "\n",
    "In summary, embeddings are essential in converting discrete tokens into continuous vectors that the model can process, while softmax converts model outputs into token probabilities. Parameter sharing between embedding layers and the pre-softmax linear transformation reduces the model's parameter count and improves training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6402fe-b6cf-4a0a-bdb7-894e57a464bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445d06d-bfeb-4d64-82b6-5bda123bc859",
   "metadata": {},
   "source": [
    "Positional Encoding\n",
    "\n",
    "Purpose:\n",
    "\n",
    "-Since Transformers lack recurrence and convolution, they need a way to understand the order of tokens in a sequence.\n",
    "-Positional encodings are added to the input embeddings to provide information about the relative or absolute position of tokens in a sequence.\n",
    "\n",
    "Method:\n",
    "\n",
    "-The positional encodings are vectors of the same dimension (d model) as the embeddings, allowing them to be added together.\n",
    "-Sine and cosine functions of different frequencies are used to generate the positional encodings.\n",
    "-Each dimension of the positional encoding corresponds to a sinusoid with a different wavelength, forming a geometric progression from 2𝜋 to 10000⋅2π.\n",
    "-The positional encoding at each position and dimension is calculated using sine and cosine functions as described in the equations provided.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "-The PositionalEncoding class implements the positional encoding function.\n",
    "-It takes parameters for the model dimension, dropout rate, and maximum sequence length.\n",
    "-Positional encodings are pre-computed once and stored as a buffer.\n",
    "-During the forward pass, positional encodings are added to the input embeddings and passed through a dropout layer.\n",
    "\n",
    "Visualization:\n",
    "\n",
    "-An example visualization is provided to demonstrate how the positional encoding varies across different positions and dimensions.\n",
    "-It shows the sinusoidal waveforms generated for a specific range of dimensions, plotted against the positions in the sequence.\n",
    "\n",
    "Comparison with Learned Positional Embeddings:\n",
    "\n",
    "-The paper mentions that learned positional embeddings were also experimented with and produced similar results.\n",
    "-However, sinusoidal positional encodings were chosen because they may allow the model to generalize better to longer sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb72552-2791-423d-a399-391b2bea4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4, 5, 6, 7]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_positional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1cbbb-033e-49fd-b7ab-361729d57449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92a116-ebf9-4d8f-8669-fb0c292327d2",
   "metadata": {},
   "source": [
    "Batches and Masking:\n",
    "\n",
    "-The Batch class is used to hold a batch of data during training. It contains the source (src) and target (tgt) sentences.\n",
    "-Masking is applied to hide padding and future words in the target sequence.\n",
    "-The make_std_mask function creates a mask to hide padding and future words in the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74f71d-b520-4507-ac1d-edfc1ee0461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30e2345b-0511-4fd5-8c53-aadbd8c6d110",
   "metadata": {},
   "source": [
    "Training Loop:\n",
    "\n",
    "-The run_epoch function is used to train a single epoch of the model.\n",
    "-It iterates over the data iterator (data_iter) containing batches of training data.\n",
    "-For each batch, the model is run forward to generate predictions.\n",
    "-The loss is computed between the predictions and the actual target tokens.\n",
    "-If the mode is set to \"train\" or \"train+log\", gradients are computed and the model parameters are updated using an optimizer.\n",
    "-Gradient accumulation is used to accumulate gradients over a certain number of iterations (accum_iter) before updating the model parameters.\n",
    "-The learning rate scheduler (scheduler) is also updated after each iteration.\n",
    "-The total loss and total number of tokens processed in the epoch are tracked and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15a484-3012-4548-8993-5a535aa26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed\n",
    "\n",
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa9ab9-b7d0-4d27-a62b-faf7e0873d45",
   "metadata": {},
   "source": [
    "Training Data and Batching:\n",
    "\n",
    "The model is trained on the WMT 2014 English-German dataset, consisting of about 4.5 million sentence pairs.\n",
    "Sentences are encoded using byte-pair encoding (BPE), with a shared vocabulary of about 37,000 tokens.\n",
    "English-French training uses the larger WMT 2014 English-French dataset, consisting of 36 million sentences, with a vocabulary of 32,000 word-piece tokens.\n",
    "Batches are created by grouping together sentence pairs with approximately 25,000 source tokens and 25,000 target tokens.\n",
    "\n",
    "Hardware and Schedule:\n",
    "\n",
    "Training is performed on a single machine with 8 NVIDIA P100 GPUs.\n",
    "For the base models, each training step takes about 0.4 seconds, and training is conducted for a total of 100,000 steps or 12 hours.\n",
    "Big models have a step time of 1.0 second and are trained for 300,000 steps or 3.5 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5c8b9-21ed-4af1-b6ca-83b81c0c8ef1",
   "metadata": {},
   "source": [
    "Optimization Hyperparameters:\n",
    "\n",
    "-The Adam optimizer is utilized with specific hyperparameters:\n",
    "-Beta 1 (β1) is set to 0.9.\n",
    "-Beta 2 (β2) is set to 0.98.\n",
    "-Epsilon (ϵ) is set to 1e-9.\n",
    "-The learning rate (lrate) is varied during training according to a formula that involves the model size (d_model), the current training step (step_num), and a warm-up factor (warmup_steps).\n",
    "-The learning rate is initially increased linearly for the first warmup_steps training steps and then decreased proportionally to the inverse square root of the step number.\n",
    "-The warm-up steps are set to 4000.\n",
    "\n",
    "-Example of Learning Rate Schedule:\n",
    "\n",
    "-An example function (rate) is provided to calculate the learning rate based on the specified parameters.\n",
    "-The example_learning_schedule function demonstrates how the learning rate varies over 20,000 training steps for different combinations of model sizes and warm-up steps.\n",
    "-The learning rate curves for different model sizes and warm-up steps are plotted to visualize their behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b40f5-6385-4685-bbb7-09f21ab2013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488c0e5-318e-4e7a-8cb1-bdfdc280f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_learning_schedule():\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    # we have 3 examples in opts list.\n",
    "    for idx, example in enumerate(opts):\n",
    "        # run 20000 epoch for each example\n",
    "        optimizer = torch.optim.Adam(\n",
    "            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n",
    "        )\n",
    "        tmp = []\n",
    "        # take 20K dummy training steps, save the learning rate at each step\n",
    "        for step in range(20000):\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_learning_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2aab1f-947a-4a53-adac-8f4828322223",
   "metadata": {},
   "source": [
    "Regularization - Label Smoothing:\n",
    "\n",
    "-Label smoothing is applied during training with a smoothing value (ε_ls) set to 0.1.\n",
    "-Label smoothing helps improve accuracy and BLEU score but may hurt perplexity.\n",
    "-The LabelSmoothing class implements label smoothing using the Kullback-Leibler divergence loss.\n",
    "-Instead of using a one-hot target distribution, label smoothing creates a distribution with confidence in the correct word and distributes the rest of the smoothing mass throughout the vocabulary.\n",
    "-An example of label smoothing is provided to visualize how the mass is distributed to words based on confidence.\n",
    "-Label smoothing penalizes the model if it becomes overly confident about a given choice.\n",
    "\n",
    "Visualization of Penalization:\n",
    "\n",
    "-A function is defined to calculate the loss and visualize how it changes based on different confidence levels.\n",
    "The penalization visualization demonstrates how the loss increases as the model becomes more confident about its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215e214-94b3-43a1-86e1-24c2bb1735de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fabfdf-4397-4c57-b46d-c755fa915ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect(color=\"Blue\", opacity=1)\n",
    "        .properties(height=200, width=200)\n",
    "        .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(example_label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402bd54-c407-4299-9d2f-f796e950e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "            \"Steps\": list(range(99)),\n",
    "        }\n",
    "    ).astype(\"float\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(loss_data)\n",
    "        .mark_line()\n",
    "        .properties(width=350)\n",
    "        .encode(\n",
    "            x=\"Steps\",\n",
    "            y=\"Loss\",\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "show_example(penalization_visualization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
